---
title: "Merging"
format: html
editor: visual
---

## Merging datasets

```{r}
Sys.setlocale("LC_ALL", "UTF-8")
library(dplyr, warn.conflicts = FALSE)
library(arrow, warn.conflicts = FALSE)
library(openxlsx, warn.conflicts = FALSE)
library(assertr, warn.conflicts = FALSE)

source("src/database/startup.R")
source("src/database/query/data.R")

conns <- load_dbs()
sets <- c("ER", "FVG", "LOM", "MAR", "TAA2", "TOS", "UMB", "VDA", "PIE", "LIG", "VEN")
datasets <- c("ARPAPiemonte", "ARPAV", "ARPAL", "ARPALombardia", "ARPAUmbria", "TAA", "SIRToscana", "Dext3r", "ARPAFVG", "ARPAM", "ISAC", "SCIA")


```

```{r}
metadata <- query_checkpoint_meta(datasets, "raw", conns$data) |> collect()

groups_table <- query_parquet(fs::path_wd("db", "extra", "series_groups", sets, ext = "parquet"), filename = T) |>
    mutate(set = parse_filename(filename, TRUE)) |>
    select(-filename) |>
    collect()

network_rank_table <- tribble(
    ~dataset, ~network, ~network_rank,
    "ISAC", "ISAC", 1L, # ISAC series are always ranked first
    "ISAC", "DPC", 4L # DPC series are always ranked last
) |>
    bind_rows(
        metadata |> filter(dataset == "SCIA") |> distinct(dataset, network) |> mutate(network_rank = 3L) # SCIA series are ranked second to last
    ) |>
    bind_rows(
        metadata |> filter(!dataset %in% c("SCIA", "ISAC")) |> distinct(dataset, network) |> mutate(network_rank = 2L) # ARPA series are ranked second
    )

# VEDERE SE USARE: ASSEGNAZIONE RANKING PER LUNGHEZZA DELLA SERIE, IN MODO DA NON PRENDERE COME RIFERIMENTO UNA SERIE TROPPO CORTE
length_rank_table <- tribble(
    ~from, ~to, ~length_rank,
    0L, 365L * 4L, 2L,
    365L * 4L + 1L, Inf, 1L
) # ???

gs <- groups_table |>
    left_join(metadata |> select(dataset, sensor_key, network, sensor_last), by = c("dataset", "sensor_key"))

ranked_groups <- gs |>
    left_join(network_rank_table, by = c("dataset", "network")) |>
    group_by(set, gkey, variable) |>
    arrange(network_rank, desc(sensor_last), .by_group = TRUE) |>
    mutate(rank = row_number(), skip_correction = "ISAC" %in% network) |>
    ungroup() |>
    select(!c(sensor_last, network_rank, network, from))
```

```{r}
corrections <- fs::path_abs("./external/correzioni") |>
    fs::dir_ls(regex = regex("[^\\~]+_edit.xlsx")) |>
    purrr::map(
        .f = \(path) read.xlsx(path) |> select(sensor_key, dataset, from_sensor_keys, from_datasets, ends_with("_ok"), ends_with("_precision"), keep)
    ) |>
    bind_rows() |>
    

corrected_meta <- query_checkpoint_meta(sets, "merged", conns$data) |>
    collect() |>
    as.data.frame() |>
    full_join(corrections, by = c("sensor_key", "dataset", "from_sensor_keys", "from_datasets")) |>
    assert(not_na, c(network, sensor_key, dataset, from_sensor_keys, from_datasets)) |>
    mutate(
        lon = coalesce(lon_ok, lon),
        lat = coalesce(lat_ok, lat),
        elevation = coalesce(ele_ok, elevation),
        name = coalesce(name_ok, name)
    )
```

```{r}
source("notebooks/merging/correzioni_manuali.R")
# dynamic_merge.full(ds_path, fs::path_wd("merged_db"), ranked_groups, 10, 0L)
merged_data <- open_dataset(fs::path(fs::path_wd("merged_db"), "data"))
merged_meta <- open_dataset(fs::path(fs::path_wd("merged_db"), "meta"))
```

```{r}
merged_data |> slice_head(n = 5L) |> collect()
```

```{r}
query_checkpoint_meta(sets, "merged", conns$data) |> select(name, network, state, province_code, lon, lat, elevation, user_code)
```
```{r}
read_parquet(fs::path_wd("db", "extra", "series_groups", "ER", ext = "parquet"))
```

