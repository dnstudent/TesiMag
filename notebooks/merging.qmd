---
title: "merging"
author: "Davide Nicoli"
format: html
editor: visual
---

```{r}
setwd(fs::path_abs("~/Local_Workspace/TesiMag"))
Sys.setlocale("LC_ALL", "UTF-8")
library(dplyr, warn.conflicts = FALSE)
library(openxlsx, warn.conflicts = FALSE)
source("src/database/query/data.R")
source("src/database/startup.R")
source("src/merging/analysis.R")
source("src/merging/display.R")
source("notebooks/ds_regionali/procedure/common_steps.R")
conns <- load_dbs()
```

```{r}
meta <- query_checkpoint_meta("full", "merged_corrected", conn = conns$data) |>
  window_order(dataset, series_key) |>
  mutate(key = row_number(), sensor_key = series_key, sensor_first = NA_Date_, sensor_last = NA_Date_) |>
  compute()
data <- query_checkpoint_data("full", "merged_corrected", conn = conns$data, hive_types = list("variable" = "INT")) |>
  inner_join(meta |> select(dataset, series_key, key), by = c("dataset", "series_key"))
```

```{r}
sensor_matches <- close_matches(meta, 20000, conns$stations)
var_matches <- series_matches(data, sensor_matches, meta)
```

```{r}
involved_series <- var_matches |> pivot_longer(cols = c(key_x, key_y), names_to = "key_type", values_to = "key")
metaf <- meta |>
  semi_join(involved_series, by = "key", copy = T) |>
  compute()
# dataf <- data |> semi_join(involved_series, by = c("key", "variable"), copy = T) |>
#   to_arrow() |>
#   write_parquet("temp.parquet")
analysis <- series_matches_analysis(var_matches,
  query_parquet("temp.parquet", conns$data),
  metaf,
  matches_offsets = c(-1L, 0L, 1L)
)
```

```{r}
analysis |> write_xlsx_analysis(file.path("notebooks", "ds_nazionali", "TESIDB", "raw_analysis.xlsx"), name_x, name_y, .format = FALSE)
```

```{r}
analysis <- read.xlsx(file.path("notebooks", "ds_nazionali", "TESIDB", "raw_analysis.xlsx")) |>
  left_join(meta |> select(dataset, sensor_key, key), by = c("dataset_x" = "dataset", "sensor_key_x" = "sensor_key"), copy = T) |>
  left_join(meta |> select(dataset, sensor_key, key), by = c("dataset_y" = "dataset", "sensor_key_y" = "sensor_key"), copy = T, suffix = c("_x", "_y"))

tagged_analysis <- analysis |>
  mutate(
    tag_dupe = ((dataset_x != dataset_y) &
      (distance < 170 |
        ((valid_days_inters > 100L &
          ((!is.na(f0noint) & f0noint > 0.12) | (dataset_x != dataset_y & fsameint > 0.9))
        )))) |
      (
        (sensor_key_x == 139L & sensor_key_y == 183L) | # Monte Zoncolan
          (sensor_key_x == 150L & sensor_key_y == 190L) | # Busalla
          (sensor_key_x == 193L & sensor_key_y == 196L) | # Cabanne
          (sensor_key_x == 172L & sensor_key_y == 336L) # Santa Maddalena in Casies
      )
  ) |>
  group_by(dataset_x, dataset_y, sensor_key_x, sensor_key_y) |>
  mutate(
    tag_anydupe = any(tag_dupe)
  ) |>
  ungroup()
tagged_analysis |> write_xlsx_analysis(file.path("notebooks", "ds_nazionali", "TESIDB", "tagged_analysis.xlsx"), name_x, name_y, tag_dupe, tag_anydupe, .format = FALSE)
```

```{r}
gs <- series_groups(tagged_analysis, collect(meta), data, tag_anydupe)
```

```{r}
regional_dataset <- tribble(
  ~dataset, ~dataset_state,
  "PIE", "Piemonte",
  "TOS", "Toscana",
  "VEN", "Veneto",
  "LOM", "Lombardia",
  "VDA", "Valle D'Aosta",
  "FVG", "Friuli Venezia Giulia",
  "ER", "Emilia-Romagna",
  "LIG", "Liguria",
  "TAA2", "Trentino-Alto Adige",
  "MAR", "Marche",
)
```

````{r}
ranked_series_groups <- gs$table |>
  left_join(collect(meta) |> select(from_dataset = dataset, from_sensor_key = sensor_key, key, state_in = state), by = "key") |>
  left_join(regional_dataset, by = c("from_dataset" = "dataset")) |>
  mutate(set = "tesidb", right_state = state_in == dataset_state) |>
  rank_metadata(collect(meta), dataset_rankings = NULL, desc(right_state), desc(valid90))
```

```{r}
ranked_series_groups |> filter(gkey == 11L)
```

```{r}
meta |> semi_join(ranked_series_groups |> filter(gkey == 11L), by = c("dataset" = "from_dataset", "series_key" = "from_sensor_key"), copy = T) |> collect()
```


```{r}
duplications <- ranked_series_groups |>
  group_by(gkey) |>
  arrange(metadata_rank) |>
  mutate(duplicate_of = if_else(metadata_rank == 1L, "", first(str_c(from_dataset, from_sensor_key, sep = "/")))) |>
  ungroup() |>
  select(dataset = from_dataset, series_key = from_sensor_key, duplicate_of) |>
  distinct()
duplications |>
  left_join(meta |> select(dataset, series_key, name, series_first, series_last, valid90, lon, lat, elevation, state), by = c("dataset", "series_key"), copy = T) |>
  readr::write_csv(file.path("notebooks", "ds_nazionali", "TESIDB", "merge_duplicates.csv"))
```

```{r}
anti_join(meta, duplications, by = c("dataset", "series_key"), copy = T) |> collect()
```



```{r}
meta |> filter(dataset == "FVG", series_key == 184L) |> collect() |> View()
```

```{r}
query_checkpoint_meta("ARPAFVG", "raw") |> filter(sensor_key == 36L)
```

